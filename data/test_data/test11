Latent Dirichlet allocation (LDA) is a topic model which infers topics from a collection of text documents. LDA can be thought of as a clustering algorithm as follows:Topics correspond to cluster centers, and documents correspond to examples (rows) in a dataset.Topics and documents both exist in a feature space, where feature vectors are vectors of word counts.Rather than estimating a clustering using a traditional distance, LDA uses a function based on a statistical model of how text documents are generated.LDA takes in a collection of documents as vectors of word counts. It supports different inference algorithms via setOptimizer function. EMLDAOptimizer learns clustering using expectation-maximization on the likelihood function and yields comprehensive results, while OnlineLDAOptimizer uses iterative mini-batch sampling for online variational inference and is generally memory friendly. After fitting on the documents, LDA provides:Topics: Inferred topics, each of which is a probability distribution over terms (words).Topic distributions for documents: For each document in the training set, LDA gives a probability distribution over topics. (EM only)LDA takes the following parameters:k: Number of topics (i.e., cluster centers)maxIterations: Limit on the number of iterations of EM used for learningdocConcentration: Hyperparameter for prior over documents’ distributions over topics. Currently must be > 1, where larger values encourage smoother inferred distributions.topicConcentration: Hyperparameter for prior over topics’ distributions over terms (words). Currently must be > 1, where larger values encourage smoother inferred distributions.checkpointInterval: If using checkpointing (set in the Spark configuration), this parameter specifies the frequency with which checkpoints will be created. If maxIterations is large, using checkpointing can help reduce shuffle file sizes on disk and help with failure recovery.Note: LDA is a new feature with some missing functionality. In particular, it does not yet support prediction on new documents, and it does not have a Python API. These will be added in the future.ExamplesIn the following example, we load word count vectors representing a corpus of documents. We then use LDA to infer three topics from the documents. The number of desired clusters is passed to the algorithm. We then output the topics, represented as probability distributions over words.